# -*- coding: utf-8 -*-
"""webcam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FnmHW-sCla5M8uIY_doDk61Pdm64FhU-
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip -o "/content/drive/MyDrive/DEEPVISION.zip" -d "/content"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/DEEPVISION

!ls -R /content/DEEPVISION

!pip install gradio==4.12 opencv-python-headless==4.8.0.74

from IPython.display import display, Javascript

def start_cam():
    display(Javascript('''
        async function() {
            const stream = await navigator.mediaDevices.getUserMedia({video:true});
            const video = document.createElement('video');
            video.srcObject = stream;
            video.play();

            window.video = video;
        }
    '''))
start_cam()

import torch
import torch.nn as nn
import cv2
import numpy as np

# -------------------------
# Minimal CSRNet Architecture
# -------------------------

class CSRNet(nn.Module):
    def __init__(self):
        super(CSRNet, self).__init__()

        self.frontend = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(True),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(True),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(True),
            nn.MaxPool2d(2),
        )

        self.backend = nn.Sequential(
            nn.Conv2d(256, 512, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(512, 256, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(256, 128, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(128, 64, 3, dilation=2, padding=2), nn.ReLU(True),
        )

        self.output_layer = nn.Conv2d(64, 1, 1)

    def forward(self, x):
        x = self.frontend(x)
        x = self.backend(x)
        x = self.output_layer(x)
        return x

!ls /content

# search for .pth files under your DEEPVISION folder in Drive
!find /content/drive -type f -name "*.pth" -maxdepth 5 -print

# example: copy a file to /content (change source path to the actual file you see above)
!cp "/content/drive/MyDrive/DEEPVISION/training/CSRNet_epoch_50.pth" /content/

# verify copied file
!ls -la /content | sed -n '1,200p'

import glob, os, shutil
candidates = [
    "/content/drive/MyDrive/DEEPVISION_checkpoints/partB/partB_finetune_best.pth",
    "/content/drive/MyDrive/DEEPVISION_checkpoints/model_finetuned_quick.pth",
    "/content/drive/MyDrive/model_epoch_50.pth"
]

found = None
for c in candidates:
    if os.path.isfile(c):
        found = c
        break

# fallback: any .pth in drive (newest)
if not found:
    pths = glob.glob("/content/drive/**/*.pth", recursive=True)
    if pths:
        pths = sorted(pths, key=os.path.getmtime, reverse=True)
        found = pths[0]

if not found:
    raise FileNotFoundError("No .pth checkpoint found in the candidate list or Drive. Upload or point to your file.")
else:
    dst = "/content/" + os.path.basename(found)
    print("Copying:", found, "->", dst)
    shutil.copy(found, dst)
    checkpoint_path = dst
    print("Copied. checkpoint_path =", checkpoint_path)

# show /content listing
!ls -la /content | sed -n '1,200p'

import torch
import torch.nn as nn
from pprint import pprint

# --- Paste your full CSRNet definition here if different ---
class CSRNet(nn.Module):
    def __init__(self):
        super(CSRNet, self).__init__()
        # frontend (VGG-like)
        self.frontend = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(True),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(True),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(True),
            nn.MaxPool2d(2),
            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(True),
        )
        self.backend = nn.Sequential(
            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(512, 512, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(512, 256, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(256, 128, 3, dilation=2, padding=2), nn.ReLU(True),
            nn.Conv2d(128, 64, 3, dilation=2, padding=2), nn.ReLU(True),
        )
        self.output_layer = nn.Conv2d(64, 1, 1)

    def forward(self, x):
        x = self.frontend(x)
        x = self.backend(x)
        x = self.output_layer(x)
        return x

# --- Load checkpoint robustly ---
checkpoint_path = "/content/" + [f for f in os.listdir("/content") if f.endswith(".pth")][0]
print("Attempting to load:", checkpoint_path)

ck = torch.load(checkpoint_path, map_location='cpu')
print("\nTop-level checkpoint type:", type(ck))
if isinstance(ck, dict):
    print("Top-level keys:")
    pprint(list(ck.keys())[:20])
else:
    print("Checkpoint is raw tensor/state")

# Determine state_dict
state_dict = None
if isinstance(ck, dict):
    # common key names
    for key_name in ["state_dict", "model_state_dict", "model", "net"]:
        if key_name in ck:
            state_dict = ck[key_name]
            print("Using state dict from key:", key_name)
            break
    # if not found and dict looks like a state_dict (tensor values)
    if state_dict is None:
        sample_vals = list(ck.values())[:5]
        if all(hasattr(v, 'size') for v in sample_vals):
            # likely already a state_dict
            state_dict = ck
            print("Treating top-level dict as state_dict")
else:
    raise RuntimeError("Unhandled checkpoint format")

# strip module. prefix if present
new_sd = {}
for k, v in state_dict.items():
    nk = k
    if k.startswith("module."):
        nk = k.replace("module.", "", 1)
    new_sd[nk] = v

# Instantiate model and try to load
model = CSRNet()
missing_keys = None
unexpected_keys = None
try:
    model.load_state_dict(new_sd, strict=False)
    print("\nLoaded state_dict with strict=False successfully.")
except Exception as e:
    print("\nLoad failed with strict=False â€” printing error:")
    print(e)

# Print summary of parameters loaded / not loaded
# (if you want an exact mismatch report:)
sd_keys = set(new_sd.keys())
model_keys = set(k for k,_ in model.state_dict().items())
missing = model_keys - sd_keys
unexpected = sd_keys - model_keys
print(f"\nModel keys: {len(model_keys)}, checkpoint keys: {len(sd_keys)}")
print("Missing keys (model params not found in checkpoint):", len(missing))
print("Unexpected keys (checkpoint params not in model):", len(unexpected))
if len(missing) <= 10:
    print(sorted(list(missing)))
if len(unexpected) <= 10:
    print(sorted(list(unexpected)))

model.eval()
print("\nModel ready (on CPU). If you want GPU, .to('cuda') when available.")

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
print("Model on", device)

from IPython.display import display, Javascript
def start_cam():
    display(Javascript('''
        async function() {
            const stream = await navigator.mediaDevices.getUserMedia({video:true});
            const video = document.createElement('video');
            video.srcObject = stream;
            video.play();
            window.video = video;
        }
    '''))
start_cam()

import time
from google.colab.output import eval_js
from base64 import b64decode
from IPython.display import display
from PIL import Image
import cv2
import numpy as np

def webcam_stream(threshold=50):
    print("Webcam stream starting...")

    # JS function to capture a frame
    from IPython.display import Javascript
    js = Javascript('''
      async function takePhoto() {
        const video = window.video;
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);
        return canvas.toDataURL('image/jpeg', 0.8);
      }
    ''')
    display(js)

    while True:
        data = eval_js("takePhoto()")
        img_bytes = b64decode(data.split(',')[1])
        jpg_array = np.frombuffer(img_bytes, dtype=np.uint8)
        frame = cv2.imdecode(jpg_array, cv2.IMREAD_COLOR)

        # ---- Run prediction ----
        density = predict_density(frame)
        count = density.sum()

        # ---- Make heatmap ----
        heat = make_heatmap(density)
        heat = cv2.resize(heat, (frame.shape[1], frame.shape[0]))
        overlay = cv2.addWeighted(frame, 0.5, heat, 0.5, 0)

        # ---- Alert ----
        if count > threshold:
            cv2.putText(overlay, "OVER-CROWD ALERT!", (10, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)

        cv2.putText(overlay, f"Count: {count:.1f}", (10, 80),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)

        # Display the output
        _, im_png = cv2.imencode(".png", overlay)
        display(Image.fromarray(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)))

        time.sleep(0.1)

from IPython.display import display, Javascript

def start_cam():
    display(Javascript('''
        async function() {
            const stream = await navigator.mediaDevices.getUserMedia({video:true});
            const video = document.createElement('video');
            video.srcObject = stream;
            video.play();
            window.video = video;
        }
    '''))
start_cam()

from IPython.display import display, Javascript
import time
from google.colab.output import eval_js

def start_cam(wait_seconds=10):
    """
    Start webcam in page and wait until window.video is ready.
    wait_seconds: how long to wait for the camera to become ready.
    """
    display(Javascript('''
    (async () => {
      try {
        if (!window.video) {
          const stream = await navigator.mediaDevices.getUserMedia({video:true});
          const video = document.createElement('video');
          video.style.display = "none";
          video.srcObject = stream;
          video.play();
          window.video = video;
        }
        console.log("Camera started (JS).");
      } catch(err) {
        console.error("Camera start error (JS):", err);
        throw err;
      }
    })()
    '''))
    # Wait for the video element to be defined and ready
    t0 = time.time()
    while True:
        try:
            # Check if video object exists and has dimensions
            ready = eval_js("!!(window.video && window.video.videoWidth && window.video.videoHeight)")
            if ready:
                print("Camera is ready.")
                return True
        except Exception as e:
            # JS might throw until permission granted
            # print("Waiting for camera permission / readiness...", e)
            pass
        if time.time() - t0 > wait_seconds:
            print(f"start_cam(): timed out after {wait_seconds}s. Make sure you allowed camera access in the browser.")
            return False
        time.sleep(0.5)

# Run it:
ok = start_cam(wait_seconds=15)
if not ok:
    print("If you see a browser permission dialog, accept it and run start_cam() again.")

import cv2, numpy as np, time
from base64 import b64decode
from IPython.display import display, clear_output
from PIL import Image
from google.colab.output import eval_js

def webcam_stream(threshold=50, fps_delay=0.15, max_retries=5):
    """
    Robust webcam stream for Colab:
    - waits for JS video to be ready
    - retries on JS eval errors
    - shows overlay image frames in notebook output
    - stop with KeyboardInterrupt (Ctrl+C) in cell
    """
    print("Starting webcam_stream()... Press Ctrl+C to stop.")
    # ensure camera started
    started = start_cam(wait_seconds=15)
    if not started:
        print("Camera not started. Run start_cam() and allow camera permission, then try again.")
        return

    retry = 0
    try:
        while True:
            try:
                data = eval_js("""
                    (async () => {
                      if (!window.video) return null;
                      const v = window.video;
                      const canvas = document.createElement('canvas');
                      canvas.width = v.videoWidth;
                      canvas.height = v.videoHeight;
                      canvas.getContext('2d').drawImage(v, 0, 0);
                      return canvas.toDataURL('image/jpeg', 0.8);
                    })()
                """)
                if data is None:
                    raise RuntimeError("window.video not ready (returned null).")
                img_bytes = b64decode(data.split(',')[1])
                jpg_array = np.frombuffer(img_bytes, dtype=np.uint8)
                frame = cv2.imdecode(jpg_array, cv2.IMREAD_COLOR)
                if frame is None:
                    raise RuntimeError("Failed to decode frame.")
                retry = 0  # success -> reset retry counter

                # ---- Run your prediction functions here ----
                # ensure predict_density and make_heatmap exist in notebook (from earlier cells)
                try:
                    density = predict_density(frame)   # expects your model + preprocess funcs defined
                except NameError:
                    print("predict_density not found. Make sure to run the cells that define your model and prediction functions.")
                    return
                except Exception as e:
                    print("Error during predict_density:", e)
                    return

                count = float(density.sum())
                heat = make_heatmap(density)  # expects a 2D density array
                heat = cv2.resize(heat, (frame.shape[1], frame.shape[0]))
                overlay = cv2.addWeighted(frame, 0.5, heat, 0.5, 0)

                if count > threshold:
                    cv2.putText(overlay, "OVER-CROWD ALERT!", (10, 40),
                                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 2)
                cv2.putText(overlay, f"Count: {count:.1f}", (10, 80),
                            cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2)

                # display
                clear_output(wait=True)
                display(Image.fromarray(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)))

                time.sleep(fps_delay)

            except Exception as e:
                retry += 1
                print(f"Frame capture error (attempt {retry}/{max_retries}): {e}")
                if retry >= max_retries:
                    print("Too many consecutive errors. Aborting stream.")
                    break
                time.sleep(0.8)
    except KeyboardInterrupt:
        print("Stream stopped by user (KeyboardInterrupt).")
    except Exception as e:
        print("Unexpected error:", e)
    finally:
        clear_output()
        print("webcam_stream() finished.")

webcam_stream()

!pip install -q --upgrade gradio

import gradio as gr
print("gradio version:", gr.__version__)

import gradio as gr
print(gr.__version__)

!pip install -q gradio==3.50.2

import gradio as gr
print(gr.__version__)

from IPython.display import display, Javascript
display(Javascript("""
(async () => {
  const stream = await navigator.mediaDevices.getUserMedia({video:true});
  const video = document.createElement('video');
  video.srcObject = stream;
  await video.play();
  window.video = video;
})();
"""))

from IPython.display import Javascript, display

display(Javascript("""
(async () => {
  const stream = await navigator.mediaDevices.getUserMedia({ video: true });
  const video = document.createElement('video');
  video.srcObject = stream;
  await video.play();
  window.video = video;
  console.log("Webcam started");
})();
"""))

import cv2, time, numpy as np
from google.colab.output import eval_js
from base64 import b64decode
from IPython.display import display, clear_output
from PIL import Image

THRESHOLD = 50

def webcam_stream():
    while True:
        try:
            data = eval_js("""
            (function(){
              if (!window.video) return null;
              const c = document.createElement('canvas');
              c.width = video.videoWidth;
              c.height = video.videoHeight;
              c.getContext('2d').drawImage(video,0,0);
              return c.toDataURL('image/jpeg',0.8);
            })()
            """)
            if data is None:
                print("Webcam not ready")
                time.sleep(1)
                continue

            img = b64decode(data.split(',')[1])
            frame = cv2.imdecode(np.frombuffer(img,np.uint8), cv2.IMREAD_COLOR)

            # ---- CSRNet inference ----
            density = predict_density(frame)
            count = density.sum()

            heat = make_heatmap(density)
            heat = cv2.resize(heat,(frame.shape[1],frame.shape[0]))
            overlay = cv2.addWeighted(frame,0.5,heat,0.5,0)

            if count > THRESHOLD:
                cv2.putText(overlay,"OVER-CROWD ALERT!",(10,40),
                            cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)

            cv2.putText(overlay,f"Count: {count:.1f}",(10,80),
                        cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)

            clear_output(wait=True)
            display(Image.fromarray(cv2.cvtColor(overlay,cv2.COLOR_BGR2RGB)))
            time.sleep(0.1)

        except KeyboardInterrupt:
            print("Stopped")
            break

webcam_stream()

